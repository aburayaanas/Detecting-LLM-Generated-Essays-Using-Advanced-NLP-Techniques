# Unmasking AI: Text Authenticity Analysis

## Detecting LLM-Generated Essays Using Advanced NLP Techniques

### Overview
Unmasking AI is our graduation project aimed at identifying essays generated by Large Language Models (LLMs) using advanced Natural Language Processing (NLP) techniques. This research-driven project applies cutting-edge methodologies to differentiate human-written texts from AI-generated ones.

### Project Timeline
**Oct 2023 â€“ Present**

### Methodology
To ensure the effectiveness and accuracy of our model, we followed a structured approach:

1. **Data Collection**
   - We utilized three extensive text-only datasets, consisting of both human-written and LLM-generated essays.
   - Data preprocessing techniques were applied to clean and normalize the text.

2. **Feature Engineering**
   - **BERT Embeddings**: Extracted contextual representations of words and phrases.
   - **Sentiment Scores**: Analyzed emotional tone and subjective characteristics.
   - **Text Readability**: Assessed complexity using readability metrics.
   - **POS (Part-of-Speech) Tagging**: Identified grammatical structures within text samples.
   - **Repetitiveness Analysis**: Measured redundancy patterns in AI-generated vs. human-written texts.

3. **Hypothesis-Driven Feature Selection**
   - Each feature selection was backed by statistically supported hypotheses, ensuring a robust foundation for model training.
   - Statistical tests and exploratory data analysis (EDA) were conducted to validate feature importance.

4. **Model Development**
   - Experimented with multiple machine learning and deep learning models.
   - Fine-tuned transformer-based architectures like BERT.
   - Evaluated performance metrics such as accuracy, precision, recall, and F1-score.

5. **Results & Insights**
   - Comparative analysis of different NLP techniques.
   - Validation of the modelâ€™s effectiveness in distinguishing AI-generated content from human-written text.

### Technologies Used
- **Programming Language**: Python
- **NLP Libraries**: TensorFlow, PyTorch, Hugging Face Transformers, spaCy, NLTK
- **Data Processing**: Pandas, NumPy, Scikit-learn
- **Statistical Analysis**: SciPy, Matplotlib, Seaborn

### Future Work
- Expanding dataset size to improve generalization.
- Exploring adversarial training methods to enhance robustness.
- Deploying the model into a web-based or API-driven application for real-world use cases.

### Contributors
- **[Your Name]** - [Your Role]
- **Project Team Members**

### Repository Structure
```
ðŸ“‚ UnmaskingAI
 â”£ ðŸ“‚ data               # Dataset files
 â”£ ðŸ“‚ models             # Trained models and configurations
 â”£ ðŸ“‚ notebooks          # Jupyter notebooks for experimentation
 â”£ ðŸ“‚ src                # Source code for preprocessing, training, and evaluation
 â”£ ðŸ“œ README.md          # Project documentation
 â”£ ðŸ“œ requirements.txt   # Dependencies
 â”— ðŸ“œ LICENSE            # License file
```

### Installation & Usage
#### Clone the Repository
```bash
git clone https://github.com/yourusername/UnmaskingAI.git
cd UnmaskingAI
```
#### Install Dependencies
```bash
pip install -r requirements.txt
```
#### Run the Model
```bash
python src/train_model.py
```

### License
This project is licensed under the [MIT License](LICENSE).

### Contact
For any inquiries, please reach out to [Your Email] or open an issue in this repository.

---

Feel free to contribute, suggest improvements, or fork this repository!
